# `Requests` (Python module)

La librería más usada para hacer solicitudes HTTP es `requests`. Rápida, ligera e ideal para interaccinoes simples y controladas.

Es una librería que permite realizar solicitudes muy personalizables a nivel de *headers*, *cookies*, *sesiones*, etc. y se encuentra muy bien recomendada para el uso contra **APIs** o scraping de **contenido estático**.

Pero así mismo cuenta con una serie de desventajas, desde la falta de soporte de JavaScript y navegación dinámica... hasta la falta de simulación de un navegador real (Lo que puede activar protecciones de *bot detection*).

- Ejemplo:
```python
import requests

req = requests.get("https://dobliuw.com", headeres={"User-Agent": "Mozilla/5.0"})
print(req.text)
```

----
# `BeautifulSoup` (Python module)

Esta libreria de Python se suele ver convinada de la mano de la librería `requests` para parsear HTML de forma sencilla.

Permite buscar por tags, clases, atributos o selectores CSS y es una librería simple, liviana y perfecta para contenido HTML estático.

Al igual que la anterior, esta no cuenta con soporte para JavaScript y el parsing puede ser más lento que con otras herramientas como `lxml`.

- Ejemplo:
```python
from bs4 import BeautifulSoup
import requests

req = requests.get("https://dobliuw.com")
to_parse = BeautifulSoup(req.text, "html.parser")

print(to_parse.find('a').get('href'))
```

---
# `Selenium` (Tool)

`Selenium` es una herramienta de código abierto que permite automatizar pruebas de navegadores web (Chrome, Firefox, Edge). Permite interactuar con elementos DOM, JS dinámico y emular acciones humanas.

Esta herramienta es ideal para sitios protegidos con JS, permite interactuar con captchas, forms y popups, así como también soporta el uso de proxies, cookies, user-agents, etc.

En este caso, es pesado y lento junto a su facil detección sin evasión.

- Ejemplo:
```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.get("https://dobliuw.com")

print(driver.title)
```

----
# `Playwrigth`(Python module)

Esta librería es una de automatización de código abierto desarrollada por Microsoft para pruebas de navegadores y scraping web. Permite automatizar interacciones con nabegadores a través de una única API. Es más sigiloso que `Selenium`.

Soporte bypass de bot detections comunes, más rápido y menos detectable que Selenium así como también permite interceptar solicitudes siendo esto ideal para el análisis de tráfico JS/API. Soporta múltiples contextos, evasión de detección, navegación por tabs, manipulación de JS y cookies.

En caso de hacer uso de modulos como `requests` una de sus desventajas es su curva de aprendizaje de la mano con que puede ser overkill si solo hace falta llevar a cabo tareas de scraping básicas o simples.

- Ejemplo:
```python
from playwrigth.sync_api import sync_playwrigth

with sync_playwrigth() as p:
	browser = p.chromium.launch(headless=False)
	page = browser.new_page()
	page.goto("https://dobliuw.com")
	print(page.title())
```

----
# `httpx` (Python module)

Una alternativa moderna a `requests` es la librería `httpx`, con soporte para HTTP/2, multiplexing, async/await y cookies persistentes.

Es ideal para scraping rápido y concurrente y conlleva menos consumo de recursos que `playwrigth`.

No soporte JavaScript y es más complejo para parsing que `BeautfulSoup`.

---
# Which to choose?

| Usage                                    | Tools or Libraries recommended      |
| ---------------------------------------- | ----------------------------------- |
| Sitio estático (HTML Puro)               | `requests` + `BeautifulSoup`        |
| APIs públicas o endpoints JSON/XML       | `httpx` o `requests`                |
| Sitio dinámico con JavaScript            | `Playwrigth` o `Selenium`           |
| Sitios con [[Cloudflare]] (JS Challenge) | `Playwrigth` (Con evasión)          |
| Automatización en pruebas ofensivas      | `Playwrigth` + proxy Burp Suite     |
| Captura para análisis de tráfico         | Burp Suite (Modo manual o Headless) |
