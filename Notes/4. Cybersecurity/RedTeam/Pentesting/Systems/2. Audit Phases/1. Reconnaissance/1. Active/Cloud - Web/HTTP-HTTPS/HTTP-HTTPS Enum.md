# HTTP/HTTPS Web Enumeration

La fase de descubrimiento es esencial en el pentesting de sitios web, ya que permite recopilar informaci´no sobre el objetivo, identificar posibles vectores de ataque y planificar las etapas posteriores.

----
# Subdomains Enumeration

Identificar subdominios activos puede ayudarnos a ampliar nuestra superficie de ataque. Para esta fase podríamos hacer uso de herramientas como: 

 - `gobuster`. 
 -  `ffuf`. 
 - `amass`.
 - `Suiblist3r`.
 - `feroxbuster`.

- Ejemplo `Gobuster` :
```bash
gobuster vhost -u http://maindomain.com -t 100 -w /usr/share/wordlists/rockyou.txt --append-domain 
```

- Ejemplo `ffuf`:
```bash
ffuff -c -u http://maindomain.com -H "HOST: FUZZ.maindomain.com" -w /usr/share/wordlists/rockyou.txt -fc 301
```

Algunos **wordlists** interesantes a utilizar en este punto podrían ser los del proyecto de [ Seclists](https://github.com/danielmiessler/SecLists) como */SecLists/Discovery/DNS/subdomains-top1million-20000.txt*.

----
# Resources Enumeration

En este punto intentamos realizar un *mapeo de la estructura del sitio* a la par que entender cómo está organizada la aplicación web, identificando rutas, directorios, archivos y puntos de entrada.

Podemos llevar a cabo la utilización de herramientas similares a la fase de enumeración de subdominios, ya que el fin es el mismo:

 - `gobuster`.
 - `wfuzz`.
 - `ffuf`. 
 - `Sublist3r`.

- Ejemplo `gobuster`:
```bash
gobuster dir -u http://maindomian.com -w /usr/share/wordlists/rockyou.txt -t 100 -x php,js,conf,bak
```

- Ejemplo `Sublist3r`:
```bash
feroxbuster -u http://maindomain.com -w /usr/share/wordlists/rockyou.txt
```

- Ejemplo `ffuf`:
```bash
ffuf -c -u http://maindomain.com/FUZZ -w /usr/share/wordlists/rockyou.txt
```

Algunas rutas de **wordlists** interesantes en este punto son: 

- */SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt* (Para búsqueda de directorios o archivos con extensión)
- */SecLists/Fuzzing/fuzz-Bo0oM.txt* (Para búsqueda de archivos como **.htaccess**, **.git**, etc.)

----
# Git Repositories Recovery

En algunos casos, un servidor web puede dejar expuesta la carpeta `/.git`, permitiendo a un atacante descargar el repositorio completo y acceder al historial de commits, configuraciones y potencialmente información sensible como contraseñas o claves.

En caso de haber llevado a cabo una enumeración de directorios y habernos encontrado con una respuesta exitosa de parte del servidor web en donde se nos indica que (Por lo general) el sitio, por ejemplo http://maindomain.com/.git/, obtuvo una respuesta *301* estaríamos frente a uno de estos casos.

En este punto podemos hacer uso de herramientas como [githack](https://github.com/lijiejie/GitHack) o [gitdumper](https://github.com/HoLLy-HaCKeR/git-dumper), las cuales se encargarán de automatizar la descarga y reconstrucción del repositorio git.

- Ejemplo con `githack`:
```bash
python3 githack.py http://maindomain.com/.git/ {/path/to/destination/repository}
```

- Ejemplo con `gitdumper`:
```bash
python3 git_dumper.py http://maindomain.com/.git/ {/path/to/destionation/repository}
```

Una vez reconstruido el repositorio en el path de destino indicado (*/path/to/destination/repository*), quedaría llevar a cabo un análisis de este repositorio, lo que, como mencionamos anteriormente, consiste en la búsqueda de credenciales harcodeadas, archivos de configuración, recursos expuestos de los cuales podemos tener el código fuente para buscar un posible vector de explotación, análisis en los historial de commits y sus respectivos cambios, etc:

- Revisar *historial de commits*:
```bash
git log
```

- Analizar *cambios en un commit*:
```bash
git show {commit_hash}
```

- Buscar palabras claves:
```bash
grep -ir 'password' ./
```

----
# Creating a Custom Wordlists

Como vimos, esta fase incluye técnicas y herramientas para recopilar información sobre el objetivo, explorar configuraciones, etc. En este caso se suman herramientas para la creación de *diccionarios personalizaods* basados en un Scrap a la web o parámetros propios:

- `cewl`.
- `crunch`

En el caso de la herramienta `cewl` haremos uso de esta para generar diccionarios perosnalizados a paritr del contendio de un sitio web, que luego podría emplearse para ataques de fuerza bruta o procesos de enumeración previamente mencionados en este artículo.

- Instalación:
```cewl
sudo apt install cewl
```

- Ejemplo generando diccionario del sitio web objetivo:
```bash
cewl -d 2 -m 5 -w custom_dict.txt http://maindomain.com
```

-  `-d`: Profundidad en el scraping.
- `-m`: Longitud mínima de las palabras.

En el caso de la herramienta `crunch` también nos servirá para la creación de un diccionario personalizado, pero en este caso basandonos en parámetros y reglas específicas.

- Instalación:
```bash
sudo apt install crunch
```

- Ejemplo 1:
```bash
crunch 8 12 abcdef1234 -o custom_dict.txt
```

- `8 12` : Longitud mínima y máxima de las cadenas.
- `abcdef1234`: Conjutno de caracteres permitidos.

- Ejemplo 2:
```bash
crunch 6 6 -t abc%%% >> custom_dict.txt
```

- `-t`: Define un patrón (Los `%` son caracteres a completar)

----
# HTTP Secure

En caso de estar enfrentandonos al protocolo *Hyper Text Transfer Protocol **Secure***, se nos suman algunos posibles puntos de interes de cara a la fase de enumeración del sitio web.. los **certificados**. Hacer uso de herramientas como `openssl` o `sslscan` nos permite analizar configuraciones **SSL/TLS** para buscar posibles debilidades o vulnerabilidades criptográficas, a la par que verificar certificados.
#### Certificados SSL 

Los certificados de **Secure Sockets Layer** (Capa de socket seguros o **SSL**) son un tipo de documento digital firmado por una entidad llamada Autoridad de certificación que asocia una clave pública a unos datos que representan la identidad de una entidad que poseee la clave privada asociada a dicha clave pública. Es decir, es un certificado digital que autentica la identidad de un sitio web y habilita una conexión cifrada. 

Cuando un navegador accede a un sitio web con SSL, el servidor presenta su certificado, que contiene una clave pública. El navegador utiliza esta clave para cifrar los datos que enviará al servidor. El servidor, a su vez, utiliza una clave privada para descifrar los datos cifrados y responder de manera segura al navegador. 

En resumen, el cerficado SSL mantiene seguras las conexiones a Internet y evita que los atacantes lean o modificquen la información transferida entre dos sistemas. 

Volviendo con el escaneo de estos, en el caso de la herramienta `openssl` nos permitirá buscar debilidades a la par que verificar certificados.

- Ejemplo para verificar la conexión SSL de un servidor:
```bash
openssl s_client -connect {target}:443
```

- Ejemplo para listar los métodos soportados por el servidor:
```bash
openssl s_client -connect {target}:443 -cipher {ciphers}
```

Se podría probar algoritmos específicos como `TLS_AES_256_GCM_SHA384`.

- Verificar detalles de un certificado:
```bash
openssl x509 -in cert.pem -text -noout
```

En el caso de la herramienta `sslscan`, esta es una herramienta más automatizada para realizar auditorías de **SSL/TLS**.

- Ejemplo de escaneo de objetivo:
```bash
sslscan {target}
```

Esto detectará versiones de SSL/TLS y ciphers habilitados, a la par que reportara vulnerabilidades como soporte de SSLv3 o ciphers débiles.